​	分布式存储系统中面临着的首要问题就是如何将大量的数据分布在不同的存储节点上，无论上层接口是KV存储、对象存储、块存储或者是列存储，在这个问题上大致是一样的。

# 指标

数据分布算法有两个基本目的：

- 均匀性：不同存储节点的负载应该均衡
- 稳定性：每次一个key通过数据分布算法得到的分布结果应该保持基本稳定，即使再有存储节点发生变化的情况下。

除以上两个基本目标外，工程中还需要从以下几个方面考虑数据分布算法的优劣。

- 性能可扩展性：这个主要考虑算法相对于存储节点规模的事件复杂度，为了整个系统的可扩展性，数据分布算法不应该在集群规模扩大后显著的增加运行时间。
- 考虑节点异构：实际工程中，不同存储节点之间可能会有很大的性能或容量差异，好的数据分布算法应该能很好的应对这种异构，提供加权的数据均匀。
- 隔离故障域：为了数据的高可用，数据分布算法应该为每个key找到一组存储节点，这些节点可能是数据的镜像副本，也可能是类似擦除码的副本方式。数据分布算法应该尽量隔离这些副本的故障域。

# 演进

## 1、Hash

一种简单直观的想法是直接用Hash来计算，简单的以key做Hash之后对节点数取模。这样可以保证**均匀性** 但一旦有节点加入或退出，所有原有节点都会收到影响，稳定性无从谈起。

## 一致性Hash

![Hash环](https://upload-images.jianshu.io/upload_images/530927-586f96c707e9441c.png)

一致性Hash可以很好的解决稳定性问题，可以将所有的存储节点排列在首尾相接的Hash环上，每个Key在计算Hash后会顺时针找到首先遇到的一组存储节点存放。而当有节点加入或退出时，仅影响该节点在Hash环上顺时针相邻的后续节点，将数据从该节点接受或者给予。但这又带来均匀性问题。即使可以将存储节点等距排列，也会在存储节点个数变化时带来数据的不均匀。而这种可能成倍数的不均匀在实际工程中是不可接受的

## 3. 带负载上限的一致性Hash

一致性Hash有节点变化时带来的不均匀的问题可以用Bounded Loads来控制。简单来说就是给Hash环上的每个节点一个负载上限为1 + e的平均负载，这个e可以自定义，当key在Hash环上顺时针找到合适节点后，会判断这个节点的负载是否已经到达上限，如果以达上限，则需要继续找之后的节点进行分配。

![](https://upload-images.jianshu.io/upload_images/530927-08cab154efe584c1.png)

如上图所示，假设每个桶当前上限是2，红色的小球按序号访问，当编号为6的红色小球到达时，发现顺时针首先遇到的B（3，4），C（1，5）都已经达到上限，因此最终放置在桶A。这个算法最吸引人的地方在于当有节点变化时，需要迁移的数据量是1/e^2相关，而与节点数或数据数均无关，也就是说当集群规模扩大时，数据迁移量并不会随着显著增加。另外，使用者可以通过调整e的值来控制均匀性和稳定性之间的权衡。无论是一致性Hash还是带负载限制的一致性Hash都**无法解决节点异构的问题**。



#### 

## 4. 带虚拟节点的一致性Hash



为了解决负载不均匀和异构问题，可以在一致性Hash 的基础上引入虚拟节点，即Hash环上的每个节点并不是实际的存储节点，而是一个虚拟节点。实际的存储节点根据不同权重，对应一个或多个虚拟节点，所有落到相应虚拟节点上的key都有该存储节点负责。

![](https://upload-images.jianshu.io/upload_images/530927-6fe3dc34df69281a.png)

如上图所示，存储节点A负责(1,3]，(4,8]，(10, 14]，存储节点B负责(14,1]，(8,10]。

但这个算法的问题在于，一个实际存储节点的加入或在退出，会影线多个虚拟节点的重新分配，进而影响很多节点参与数据迁移中。

## 5.分片

分片将Hash环切割为相同大小的分片，然后将这些分片交给不同节点负责。注意这里跟上面提到的虚拟节点有着很本质的区别，**分片的划分和分片的分配被解耦**，一个节点退出时，其所负责的分片并不需要顺时针合并给之后节点，而是可以更灵活的将整个分片作为一个整体交给任意节点，实践中，一个分片多作为最小的数据迁移和备份单位。



![](https://upload-images.jianshu.io/upload_images/530927-b5bc316770ad0d69.png)

而也正是由于上面提到的解耦，相当于将原先的key到节点的映射拆成两层，需要一个新的机制来进行分片到存储节点的映射，由于分片数相对key空间已经很小并且数量确定，可以更精确地初始设置，并引入中心目录服务来根据节点存活修改分片的映射关系，同时将这个映射信息通知给所有的存储节点和客户端

## CRUSH算法

CRUSH算法本质上也是一种分片的数据分布方式，其试图在以下几个方面进行优化：

- **分片映射信息量**：避免中心目录服务和存储节点及客户端之间需要交互大量的分片映射信息，而改由存储节点或客户端自己根据少量且稳定的集群节点拓扑和确定的规则自己计算分片映射。
- **完善的故障域划分**：支持层级的故障域控制，将同一分片的不同副本按照配置划分到不同层级的故障域中。

客户端或存储节点利用key、存储节点的拓扑结构和分配算法，独立进行分片位置的计算，得到一组负责对应分片及副本的存储位置。如下图所示是一次定位的过程，最终选择了一个row下的cab21，cab23，cab24三个机柜下的三个存储节点。

![](https://upload-images.jianshu.io/upload_images/530927-35a1aa457f2b4dee.png)

当节点变化时，由于节点拓扑的变化，会影响少量分片数据进行迁移，如下图新节点加入是引起的数据迁移，通过良好的分配算法，可以得到很好的负载均衡和稳定性，CRUSH提供了Uniform、List、Tree、Straw四种分配算法。

![](https://upload-images.jianshu.io/upload_images/530927-ce71890157d0524b.png)

## 应用

- Dynamo及Cassandra采用分片的方式并通过Gossip在对等节点间同；
- Redis Cluster将key space划分为slots，同样利用Gossip通信；
- [Zeppelin](https://link.jianshu.com?t=https://github.com/Qihoo360/zeppelin)将数据分片为Partition，通过Meta集群提供中心目录服务；
- Bigtable将数据切割为Tablet，类似于可变的分片，Tablet Server可以进行分片的切割，最终分片信息记录在Chubby中；
- Ceph采用CRUSH方式，由中心集群Monitor维护并提供集群拓扑的变化。

